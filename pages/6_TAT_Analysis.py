# -*- coding: utf-8 -*-
"""QC_TAT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14BBhNywnrGpcKGML2UedY_qylSTL4CLP
"""

# pages/6_TAT_Analysis.py
# ----------------------
# TAT (Turnaround Time) Breach Analysis ‚Äî Page 6 for Streamlit app
# - Filters Location == "BAVLA"
# - Allows MatType checkbox filtering
# - Calendar date-range selection (Testing Slip Date used to bucket & select)
# - TAT Deadline = Testing Slip Date + 17 days, breaches when Final Released date - Testing Slip Date > 17
# - Weekly breach % (cohort = tests requested in that week)
# - Download Excel of breached items with requested columns
# ----------------------

import io
import os
import re
from datetime import timedelta, date
import pandas as pd
import numpy as np
import streamlit as st
import plotly.express as px

# Try reportlab or openpyxl/xlrd won't be needed for reading/writing if environment has openpyxl
# We'll attempt to read using openpyxl first, then xlrd. If both unavailable, user will see clear message.

st.set_page_config(page_title="TAT Breach Analysis", page_icon="‚è±Ô∏è", layout="wide")
st.title("‚è±Ô∏è TAT Breach ‚Äî Quality (BAVLA)")
st.caption("Filter by MatType and date range. TAT Deadline = Testing Slip Date + 17 days. Weekly breach % uses requests (Testing Slip) week.")

# ----------------- canonical column names (exact as you asked) -----------------
COL_LOCATION           = "Location"
COL_ITEM_NAME          = "Item Name"
COL_INWARD_BATCH       = "Inward No. / Batch No."
COL_MATTYPE            = "MatType"
COL_TEST_DATE          = "Testing Slip Date"
COL_FINAL_RELEASE_DATE = "Final Released date"

REQUIRED_COLS = [
    COL_LOCATION, COL_ITEM_NAME, COL_INWARD_BATCH, COL_MATTYPE,
    COL_TEST_DATE, COL_FINAL_RELEASE_DATE
]

TAT_LIMIT_DAYS = 17
LOCATION_KEEP = "BAVLA"

# ----------------- helpers -----------------
def read_excel_try_engines(file_like):
    """
    Try reading uploaded file-like with openpyxl then xlrd.
    Returns dataframe or raises informative Exception for user instruction.
    """
    # ensure pointer at start
    try:
        file_like.seek(0)
    except Exception:
        pass

    errors = []
    for eng in ("openpyxl", "xlrd"):
        try:
            df = pd.read_excel(file_like, engine=eng)
            return df
        except Exception as e:
            errors.append((eng, str(e)))
            # rewind for next attempt (UploadedFile or BytesIO)
            try:
                file_like.seek(0)
            except Exception:
                pass
    # If we reached here, none worked
    msg = "Unable to read the uploaded Excel file. Attempts:\n"
    for eng, err in errors:
        msg += f" - engine='{eng}': {err}\n"
    msg += "\nIf your file is .xlsx ensure 'openpyxl' is installed; if .xls install 'xlrd'.\nExample: pip install openpyxl xlrd"
    raise ValueError(msg)

def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    Map common column-name variants to canonical names required by this page.
    This preserves other columns unchanged.
    """
    def key(s):
        s = str(s).strip()
        s = re.sub(r"\u00A0", " ", s)
        s = re.sub(r"\s+", " ", s)
        # remove punctuation/spaces for fuzzy matching
        return re.sub(r"[^A-Za-z0-9]", "", s).lower()

    mapping = {}
    for col in df.columns:
        k = key(col)
        if k in ("location",):
            mapping[col] = COL_LOCATION
        elif k in ("itemname","materialname","productname","item"):
            mapping[col] = COL_ITEM_NAME
        elif k in ("inwardnobatchno","inwardnobatch","inwardnobatchno","batchno","inwardno","inwardbatchno"):
            mapping[col] = COL_INWARD_BATCH
        elif k in ("mattype","materialtype","mat_type","mattypes"):
            mapping[col] = COL_MATTYPE
        elif k in ("testingslipdate","testslipdate","testingdate","slipdate","testsdate"):
            mapping[col] = COL_TEST_DATE
        elif k in ("finalreleaseddate","finalreleaseddt","releaseddate","finalrelease","releasedon"):
            mapping[col] = COL_FINAL_RELEASE_DATE
        else:
            mapping[col] = col  # keep original

    df2 = df.copy()
    df2.columns = [mapping[c] for c in df.columns]
    return df2

@st.cache_data(show_spinner=True)
def load_and_prepare(uploaded_file):
    # read
    df_raw = read_excel_try_engines(uploaded_file)
    # normalize columns
    df_raw = normalize_columns(df_raw)
    # validate presence of required columns
    missing = [c for c in REQUIRED_COLS if c not in df_raw.columns]
    if missing:
        raise ValueError(f"Input file missing required columns (after normalization): {missing}. Columns found: {list(df_raw.columns)}")
    # Keep only required columns + preserve originals if needed
    df = df_raw[REQUIRED_COLS].copy()
    # Trim whitespace in text columns
    df[COL_LOCATION] = df[COL_LOCATION].astype(str).str.strip()
    df[COL_MATTYPE] = df[COL_MATTYPE].astype(str).str.strip()
    df[COL_ITEM_NAME] = df[COL_ITEM_NAME].astype(str).str.strip()
    df[COL_INWARD_BATCH] = df[COL_INWARD_BATCH].astype(str).str.strip()
    # parse dates (day-first parsing ‚Äî India)
    df[COL_TEST_DATE] = pd.to_datetime(df[COL_TEST_DATE], errors="coerce", dayfirst=True)
    df[COL_FINAL_RELEASE_DATE] = pd.to_datetime(df[COL_FINAL_RELEASE_DATE], errors="coerce", dayfirst=True)
    # drop rows missing essential dates
    df = df.dropna(subset=[COL_TEST_DATE, COL_FINAL_RELEASE_DATE])
    return df

def compute_tat(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df["TAT Deadline"] = df[COL_TEST_DATE] + pd.to_timedelta(TAT_LIMIT_DAYS, unit="D")
    df["TAT Days"] = (df[COL_FINAL_RELEASE_DATE] - df[COL_TEST_DATE]).dt.days
    df["TAT Breached"] = df["TAT Days"] > TAT_LIMIT_DAYS
    return df

def week_start_monday(ts_series):
    # returns Timestamp normalized to start-of-week Monday (00:00)
    return (ts_series - pd.to_timedelta(ts_series.dt.weekday, unit="D")).dt.normalize()

def df_to_excel_bytes(df: pd.DataFrame, sheet_name: str = "Breaches"):
    """
    Convert DataFrame to Excel in-memory bytes. If openpyxl not available this may error.
    """
    towrite = io.BytesIO()
    try:
        with pd.ExcelWriter(towrite, engine="openpyxl") as writer:
            df.to_excel(writer, index=False, sheet_name=sheet_name)
        towrite.seek(0)
        return towrite.read(), "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    except Exception:
        # fallback to CSV bytes
        csv_bytes = df.to_csv(index=False).encode("utf-8")
        return csv_bytes, "text/csv"

# ----------------- UI: file upload (reuse app uploaded file if present) -----------------
if not st.session_state.get("tat_data_file_loaded"):
    uploaded_file = st.file_uploader("üì§ Upload Excel for TAT analysis (single sheet expected)", type=["xlsx", "xls"])
else:
    uploaded_file = None

# prefer global uploaded_file if your main app already stored it (keeps consistency across pages)
if "uploaded_file" in st.session_state and st.session_state.get("uploaded_file") is not None:
    uploaded_file = st.session_state["uploaded_file"]

# if page-level upload was used, store it (so other pages can reuse if necessary)
if uploaded_file is not None:
    st.session_state["uploaded_file"] = uploaded_file
    st.session_state["tat_data_file_loaded"] = True

if not st.session_state.get("tat_data_file_loaded"):
    st.info(f"Upload your Excel. Required columns (variants accepted): {REQUIRED_COLS}")
    st.stop()

# Load and prepare data (cached)
try:
    df_all = load_and_prepare(st.session_state["uploaded_file"])
except Exception as e:
    st.error(f"‚ùå Processing error: {e}")
    st.stop()

if df_all.empty:
    st.warning("No usable rows found after basic cleaning.")
    st.stop()

# Filter to Location == BAVLA (case-insensitive)
df_all[COL_LOCATION] = df_all[COL_LOCATION].astype(str)
df_bavla = df_all[df_all[COL_LOCATION].str.upper() == LOCATION_KEEP.upper()].copy()
if df_bavla.empty:
    st.warning(f"No rows found for Location = {LOCATION_KEEP}.")
    st.stop()

# Sidebar: MatType filters
st.sidebar.header("üéõ MatType filter & date range")
all_mattypes = sorted(df_bavla[COL_MATTYPE].dropna().astype(str).unique().tolist())
select_all = st.sidebar.checkbox("Select ALL MatType", value=True)
selected_mattypes = []
if select_all:
    selected_mattypes = all_mattypes
else:
    st.sidebar.caption("Tick the MatTypes to include:")
    for m in all_mattypes:
        if st.sidebar.checkbox(m, value=False, key=f"mt_{m}"):
            selected_mattypes.append(m)

if not selected_mattypes:
    st.warning("Please select at least one MatType.")
    st.stop()

df_filtered_mt = df_bavla[df_bavla[COL_MATTYPE].astype(str).isin(selected_mattypes)].copy()
if df_filtered_mt.empty:
    st.warning("No rows remain after MatType filtering.")
    st.stop()

# Determine date-range available for Testing Slip Date
min_test_dt = df_filtered_mt[COL_TEST_DATE].min().date()
max_test_dt = df_filtered_mt[COL_TEST_DATE].max().date()

st.sidebar.write(f"Data range (Testing Slip Date): {min_test_dt} ‚Üí {max_test_dt}")

# Date range selector (calendar) ‚Äî default to full available range
start_end = st.date_input(
    "Select Start Date and End Date (based on Testing Slip Date)",
    value=(min_test_dt, max_test_dt),
    min_value=min_test_dt,
    max_value=max_test_dt,
    help="Select the range of Testing Slip Dates to include in the TAT calculation."
)

# date_input may return a single date if user picks one; ensure tuple
if isinstance(start_end, tuple) or isinstance(start_end, list):
    if len(start_end) == 2:
        start_date, end_date = start_end
    else:
        st.error("Please select a start and end date.")
        st.stop()
else:
    # single date: treat as start=end
    start_date = start_end
    end_date = start_end

# Validate that some data exists in this selected range (Testing Slip Date)
mask_range = (df_filtered_mt[COL_TEST_DATE].dt.date >= start_date) & (df_filtered_mt[COL_TEST_DATE].dt.date <= end_date)
df_range = df_filtered_mt.loc[mask_range].copy()

if df_range.empty:
    st.error("Date selected not in Data")
    st.stop()

# Compute TAT columns
df_range = compute_tat(df_range)

# Summary KPIs
total_requests = df_range.shape[0]
total_breached = int(df_range["TAT Breached"].sum())
breach_pct = (total_breached / total_requests * 100) if total_requests > 0 else 0.0

k1, k2, k3 = st.columns(3)
k1.metric("Total requests (range)", f"{total_requests}")
k2.metric("TAT Breached (count)", f"{total_breached}")
k3.metric("Breach %", f"{breach_pct:.1f}%")

# Table of breached items (year-range)
breaches_df = df_range[df_range["TAT Breached"]].copy()
# Prepare display columns and formatting
display_cols = [
    COL_ITEM_NAME, COL_INWARD_BATCH, COL_MATTYPE,
    COL_TEST_DATE, "TAT Deadline", COL_FINAL_RELEASE_DATE, "TAT Days"
]
if not breaches_df.empty:
    # format dates for display
    df_display = breaches_df[display_cols].copy()
    df_display[COL_TEST_DATE] = df_display[COL_TEST_DATE].dt.strftime("%Y-%m-%d")
    df_display["TAT Deadline"] = df_display["TAT Deadline"].dt.strftime("%Y-%m-%d")
    df_display[COL_FINAL_RELEASE_DATE] = df_display[COL_FINAL_RELEASE_DATE].dt.strftime("%Y-%m-%d")
    st.subheader("üö® TAT Breached Items (preview)")
    st.dataframe(df_display.reset_index(drop=True), use_container_width=True)
else:
    st.info("No breaches in the selected date range & MatType filter.")

# Weekly TAT breach calculation (cohort = Testing Slip week)
df_range["WeekStart"] = week_start_monday(df_range[COL_TEST_DATE])
weekly = (
    df_range.groupby("WeekStart", as_index=False)
            .agg(
                Total_Orders=(COL_ITEM_NAME, "count"),
                Breached=("TAT Breached", "sum")
            )
            .sort_values("WeekStart")
)
weekly["Breach_%"] = (weekly["Breached"] / weekly["Total_Orders"] * 100).round(1)

# Plot weekly breach %
st.subheader("üìà Weekly TAT Breach (%) ‚Äî cohort = Testing Slip week")
if weekly.empty:
    st.info("No weekly data to show for the selected range.")
else:
    fig = px.bar(
        weekly,
        x="WeekStart",
        y="Breach_%",
        text=weekly["Breach_%"].map(lambda v: f"{v:.1f}%"),
        labels={"WeekStart": "Week Start (Mon)", "Breach_%": "Breach %"},
        height=420
    )
    fig.update_traces(textposition="outside")
    fig.update_layout(margin=dict(l=20, r=20, t=40, b=20))
    st.plotly_chart(fig, use_container_width=True)
    st.write("Weekly summary:")
    st.dataframe(weekly.assign(WeekStart=weekly["WeekStart"].dt.strftime("%Y-%m-%d")).reset_index(drop=True), use_container_width=True)

# ----------------- Export breached items as Excel (Item Name, Inward No. / Batch No., TAT Deadline, Final Released date) -----------------
st.subheader("‚¨áÔ∏è Download breached items")
if not breaches_df.empty:
    # prepare export DataFrame with exactly requested columns and names
    export_cols = [
        COL_ITEM_NAME,
        COL_INWARD_BATCH,
        "TAT Deadline",
        COL_FINAL_RELEASE_DATE
    ]
    export_df = breaches_df[export_cols].copy()
    # ensure datetime columns are actual datetimes, format left to Excel
    # convert to Excel-friendly (keep as datetimes‚Äîpandas will write them)
    # Build the bytes
    try:
        excel_bytes, mime = df_to_excel_bytes(export_df, sheet_name="TAT_Breaches")
        default_filename = f"TAT_breaches_{start_date}_{end_date}.xlsx" if mime.endswith("sheet") else f"TAT_breaches_{start_date}_{end_date}.csv"
        st.download_button("Download Excel (or CSV fallback)", data=excel_bytes, file_name=default_filename, mime=mime)
    except Exception as e:
        # fallback to CSV bytes
        csv_bytes = export_df.to_csv(index=False).encode("utf-8")
        st.download_button("Download CSV", data=csv_bytes, file_name=f"TAT_breaches_{start_date}_{end_date}.csv", mime="text/csv")
else:
    st.write("No breached items to export for the selected range.")

# ----------------- Debug / info -----------------
with st.expander("üîé Debug / Info (useful)"):
    st.write("Selected MatTypes:", selected_mattypes)
    st.write(f"Testing Slip Date range available: {min_test_dt} ‚Üí {max_test_dt}")
    st.write(f"Selected date range: {start_date} ‚Üí {end_date}")
    st.write("Rows in selected range:", total_requests)
    st.write("Rows breached:", total_breached)
    st.write("Sample of source columns (first 5 rows):")
    st.dataframe(df_range.head()[[COL_LOCATION, COL_MATTYPE, COL_TEST_DATE, COL_FINAL_RELEASE_DATE]].assign(
        **{COL_TEST_DATE: lambda dfc: dfc[COL_TEST_DATE].dt.strftime("%Y-%m-%d"),
           COL_FINAL_RELEASE_DATE: lambda dfc: dfc[COL_FINAL_RELEASE_DATE].dt.strftime("%Y-%m-%d")}
    ), use_container_width=True)